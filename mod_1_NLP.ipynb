{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is Computational Linguistics and how does it relate to NLP?"
      ],
      "metadata": {
        "id": "dqkqXgr9-6Oz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Computational Linguistics (CL) is an interdisciplinary field that focuses on the scientific study and computational modeling of human language. It involves developing algorithms, models, and theories to understand linguistic structures, syntax, semantics, and phonetics using computers. CL emphasizes theoretical aspects, such as formal grammars and knowledge representation, to analyze and generate language in a way that mimics human cognitive processes.\n",
        "\n",
        "Natural Language Processing (NLP), while closely related, is more application-driven. It applies computational techniques to enable machines to process, understand, and interact with human language in practical scenarios, such as chatbots or translation systems.\n",
        "\n",
        "The relationship between CL and NLP is foundational: CL provides the theoretical underpinnings and linguistic insights that inform NLP techniques, while NLP often implements CL models in real-world technologies. In essence, CL is more about studying language computationally for knowledge, whereas NLP focuses on engineering solutions for useful tasks. Many professionals work at the intersection of both fields, with CL advancing the science that powers NLP innovations."
      ],
      "metadata": {
        "id": "a2Xeghyw-6LW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Briefly describe the historical evolution of Natural Language Processing."
      ],
      "metadata": {
        "id": "5Z0oKdhR-6JE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The field of Natural Language Processing (NLP) has evolved significantly since its inception, driven by advancements in computing, linguistics, and artificial intelligence.\n",
        "\n",
        "* 1940s-1950s: Origins in Machine Translation: NLP began in the late 1940s post-World War II, with early efforts focused on automatic translation between languages, such as Russian to English, motivated by geopolitical needs. Pioneering work included Warren Weaver's 1949 memorandum on translation using computers. The 1950s saw symbolic approaches, including Noam Chomsky's 1957 book \"Syntactic Structures,\" which introduced generative grammar and influenced rule-based systems.\n",
        "\n",
        "* 1960s-1970s: Rule-Based Systems and Early AI: The 1960s introduced conversational systems like ELIZA (1966), a simple chatbot simulating a therapist, and SHRDLU (1970), which understood natural language in a block world. The 1970s emphasized conceptual ontologies to structure real-world knowledge for language understanding. However, progress stalled due to the \"AI Winter\" from overhyped expectations and limited computing power.\n",
        "\n",
        "* 1980s-1990s: Statistical and Machine Learning Shift: The 1980s marked a transition to statistical methods, using probabilities and corpora for tasks like speech recognition. The 1990s integrated machine learning, with algorithms like Hidden Markov Models enabling data-driven approaches, fueled by increasing data availability and computational resources.\n",
        "\n",
        "* 2000s-Present: Deep Learning and Modern Era: The 2000s saw the rise of neural networks and large datasets. Key milestones include Word2Vec (2013) for word embeddings, followed by deep learning breakthroughs like RNNs, LSTMs, and Transformers (2017). Models such as BERT (2018) and GPT series (from 2018 onward) revolutionized NLP with pre-trained language models. Today, NLP incorporates multimodal data, ethical considerations, and applications in generative AI, with ongoing advancements in efficiency and multilingual support.\n",
        "\n",
        "This evolution reflects a shift from rigid rules to data-driven, learning-based systems, enabling more robust and scalable applications."
      ],
      "metadata": {
        "id": "tIVQ-QlA-6HQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. List and explain three major use cases of NLP in today’s tech industry."
      ],
      "metadata": {
        "id": "371kNSOC-6Eo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "NLP is integral to the tech industry, powering tools that enhance user experiences, automate processes, and derive insights from text data. Here are three major use cases:\n",
        "\n",
        "1. Sentiment Analysis: This involves analyzing text to determine the emotional tone, such as positive, negative, or neutral. In the tech industry, companies like Amazon and Twitter use it to gauge customer opinions from reviews, social media posts, and feedback. For example, it helps brands monitor reputation, improve products, and personalize marketing. Advanced models classify sentiments at scale, enabling real-time insights.\n",
        "\n",
        "2. Chatbots and Virtual Assistants: NLP enables conversational AI systems like Siri, Alexa, or customer service bots on websites. These systems understand user queries, generate responses, and perform tasks such as booking appointments or answering FAQs. In e-commerce and customer support, they reduce response times and operational costs while providing 24/7 availability.\n",
        "\n",
        "3. Machine Translation: Tools like Google Translate use NLP to convert text or speech from one language to another. In the global tech industry, this facilitates cross-border communication, content localization for apps/websites, and accessibility. It employs neural networks for context-aware translations, improving accuracy for business expansion and user engagement.\n",
        "\n",
        "These use cases demonstrate NLP's role in making technology more intuitive and efficient."
      ],
      "metadata": {
        "id": "E4KZt5Pa-6Bo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is text normalization and why is it essential in text processing tasks?"
      ],
      "metadata": {
        "id": "Bw8UTX9a-5_I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Text normalization is the process of converting raw text into a standardized, consistent format to facilitate analysis and modeling. It involves techniques such as:\n",
        "\n",
        "* Lowercasing all text (e.g., \"Hello\" → \"hello\").\n",
        "* Removing punctuation, special characters, or noise (e.g., hashtags, URLs).\n",
        "* Expanding contractions (e.g., \"don't\" → \"do not\").\n",
        "* Handling numbers, dates, or acronyms uniformly.\n",
        "* Correcting spelling errors or removing stop words (common words like \"the,\" \"is\").\n",
        "\n",
        "This step ensures that variations in text (e.g., \"Run,\" \"running,\" \"RUN\") are treated as the same entity.\n",
        "\n",
        "It is essential in text processing tasks because raw text from sources like social media or reviews is often inconsistent, noisy, and varied due to human input errors, abbreviations, or formatting. Without normalization, models may treat similar words differently, leading to poor performance in tasks like sentiment analysis, search, or machine learning. Normalization reduces dimensionality, improves accuracy, and enhances efficiency by creating cleaner data for downstream processes like tokenization or embedding."
      ],
      "metadata": {
        "id": "gpF9xRZr-58Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Compare and contrast stemming and lemmatization with suitable examples."
      ],
      "metadata": {
        "id": "IjIcW80F-55p"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Stemming and lemmatization are both text preprocessing techniques used to reduce words to their base or root form, but they differ in approach, accuracy, and output.\n",
        "\n",
        "**Similarities:**\n",
        "\n",
        "* Both aim to normalize words by reducing inflected forms (e.g., plurals, tenses) to a common root, helping in tasks like information retrieval or sentiment analysis.\n",
        "\n",
        "* They reduce vocabulary size, improving computational efficiency and model performance by treating variants as one.\n",
        "\n",
        "**Differences:**\n",
        "\n",
        "**Stemming:**\n",
        "\n",
        "A heuristic, rule-based process that chops off suffixes/prefixes to find the stem, often producing non-real words. It is faster but less accurate, as it doesn't consider context or part-of-speech (POS).\n",
        "\n",
        "* Algorithm: Common ones include Porter or Snowball stemmer.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* \"Running\" → \"run\"\n",
        "\n",
        "* \"Computers\" → \"comput\"\n",
        "\n",
        "* \"Better\" → \"better\" (no change, but may fail on irregulars like \"went\" → \"went\")\n",
        "\n",
        "\n",
        "Pros: Simple, quick.\n",
        "\n",
        "Cons: Can over-stem (e.g., \"university\" → \"univers\") or produce invalid words.\n",
        "\n",
        "**Lemmatization:**\n",
        "\n",
        " A more sophisticated method that reduces words to their dictionary base form (lemma), considering context, POS, and morphology. It requires linguistic knowledge and is slower but more precise.\n",
        "\n",
        "* Algorithm: Uses tools like WordNet or spaCy/NLTK lemmatizers.\n",
        "\n",
        "Examples:\n",
        "\n",
        "* \"Running\" → \"run\"\n",
        "* \"Computers\" → \"computer\"\n",
        "* \"Better\" → \"good\" (handles adjectives correctly)\n",
        "* \"Went\" → \"go\"\n",
        "\n",
        "\n",
        "Pros: Produces valid words, context-aware.\n",
        "\n",
        "Cons: Computationally intensive, needs POS tagging.\n",
        "\n",
        "In summary, stemming is crude and fast for large datasets where accuracy isn't critical, while lemmatization is preferred for tasks requiring semantic precision, like question answering."
      ],
      "metadata": {
        "id": "u8GFtSbA-52_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Write a Python program that uses regular expressions (regex) to extract all email addresses from the following block of text"
      ],
      "metadata": {
        "id": "xDcCiF5v-50I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "text = \"\"\"Hello team, please contact us at support@xyz.com for technical issues, or reach out to\n",
        "our HR at hr@xyz.com. You can also connect with John at john.doe@xyz.org and jenny\n",
        "via jenny_clarke126@mail.co.us. For partnership inquiries, email partners@xyz.biz.\"\"\"\n",
        "\n",
        "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', text)\n",
        "\n",
        "print(emails)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "urAwtCmjCxi2",
        "outputId": "639d4f9f-5d99-44a2-f628-b65349b9a025"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['support@xyz.com', 'hr@xyz.com', 'john.doe@xyz.org', 'jenny_clarke126@mail.co.us', 'partners@xyz.biz']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Given the sample paragraph below, perform string tokenization and frequency distribution using Python and NLTK"
      ],
      "metadata": {
        "id": "CIaFXC7w-5w7"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f10c952",
        "outputId": "1224ab02-ca09-40dd-d30c-d762426748ef"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.probability import FreqDist\n",
        "import os\n",
        "\n",
        "# Set the NLTK data path explicitly\n",
        "nltk.data.path.append(os.path.abspath(\".\"))\n",
        "\n",
        "# Download required data (if available in environment)\n",
        "# Check if 'punkt' is already downloaded to avoid repeated downloads\n",
        "try:\n",
        "    nltk.data.find('tokenizers/punkt')\n",
        "except nltk.downloader.DownloadError:\n",
        "    pass # Removed nltk.download('punkt') as requested\n",
        "except LookupError:\n",
        "    pass # Removed nltk.download('punkt') as requested\n",
        "\n",
        "\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "tokens = word_tokenize(text)\n",
        "fdist = FreqDist(tokens)\n",
        "\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Frequency Distribution:\", fdist.most_common())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 882
        },
        "id": "VpuXg4HZDeZR",
        "outputId": "154b7fda-e677-44d6-cd81-c22fa5b3ed97"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "\n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/content'\n    - '/content'\n    - '/content'\n    - '/content'\n    - '/content'\n**********************************************************************\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3919582354.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mtokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mword_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0mfdist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFreqDist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36mword_tokenize\u001b[0;34m(text, language, preserve_line)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mpreserve_line\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m     \"\"\"\n\u001b[0;32m--> 142\u001b[0;31m     \u001b[0msentences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mpreserve_line\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    143\u001b[0m     return [\n\u001b[1;32m    144\u001b[0m         \u001b[0mtoken\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msent\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msentences\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtoken\u001b[0m \u001b[0;32min\u001b[0m \u001b[0m_treebank_word_tokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[0;34m(text, language)\u001b[0m\n\u001b[1;32m    117\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mparam\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mPunkt\u001b[0m \u001b[0mcorpus\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \"\"\"\n\u001b[0;32m--> 119\u001b[0;31m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_punkt_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    120\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/__init__.py\u001b[0m in \u001b[0;36m_get_punkt_tokenizer\u001b[0;34m(language)\u001b[0m\n\u001b[1;32m    103\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0mtype\u001b[0m \u001b[0mlanguage\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \"\"\"\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mPunktTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlanguage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1742\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1743\u001b[0m         \u001b[0mPunktSentenceTokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1744\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1745\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1746\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mload_lang\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"english\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/tokenize/punkt.py\u001b[0m in \u001b[0;36mload_lang\u001b[0;34m(self, lang)\u001b[0m\n\u001b[1;32m   1747\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1748\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1749\u001b[0;31m         \u001b[0mlang_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"tokenizers/punkt_tab/{lang}/\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1750\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_params\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_punkt_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlang_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1751\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lang\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlang\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/nltk/data.py\u001b[0m in \u001b[0;36mfind\u001b[0;34m(resource_name, paths)\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0msep\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"*\"\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m70\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m     \u001b[0mresource_not_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"\\n{sep}\\n{msg}\\n{sep}\\n\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m     \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_not_found\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93mpunkt_tab\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('punkt_tab')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtokenizers/punkt_tab/english/\u001b[0m\n\n  Searched in:\n    - '/root/nltk_data'\n    - '/usr/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n    - '/content'\n    - '/content'\n    - '/content'\n    - '/content'\n    - '/content'\n**********************************************************************\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* since error is persisting and i have found no solution , we use another library"
      ],
      "metadata": {
        "id": "8Ow6WSCnJgUJ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1933ae30",
        "outputId": "35399cd7-e7a2-403b-c942-71e4eff90b1b"
      },
      "source": [
        "import re\n",
        "from collections import Counter\n",
        "\n",
        "# Your provided text\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "# 1. Tokenization and Normalization (in one step)\n",
        "# Use regex to find all sequences of word characters (\\w+)\n",
        "# \\b ensures we only match whole words\n",
        "# .lower() converts the whole text to lowercase first\n",
        "tokens = re.findall(r'\\b\\w+\\b', text.lower())\n",
        "\n",
        "# 2. Frequency Distribution\n",
        "# The Counter object works just like NLTK's FreqDist\n",
        "fdist = Counter(tokens)\n",
        "\n",
        "# Print the results\n",
        "print(\"--- Standard Python Results ---\")\n",
        "print(\"\\nAll Tokens:\")\n",
        "print(tokens)\n",
        "\n",
        "print(\"\\nFrequency Distribution:\")\n",
        "# Print the 10 most common words\n",
        "print(fdist.most_common(10))\n",
        "\n",
        "print(\"\\nFull Frequency List:\")\n",
        "for word, frequency in fdist.items():\n",
        "    print(f\"{word}: {frequency}\")"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Standard Python Results ---\n",
            "\n",
            "All Tokens:\n",
            "['natural', 'language', 'processing', 'nlp', 'is', 'a', 'fascinating', 'field', 'that', 'combines', 'linguistics', 'computer', 'science', 'and', 'artificial', 'intelligence', 'it', 'enables', 'machines', 'to', 'understand', 'interpret', 'and', 'generate', 'human', 'language', 'applications', 'of', 'nlp', 'include', 'chatbots', 'sentiment', 'analysis', 'and', 'machine', 'translation', 'as', 'technology', 'advances', 'the', 'role', 'of', 'nlp', 'in', 'modern', 'solutions', 'is', 'becoming', 'increasingly', 'critical']\n",
            "\n",
            "Frequency Distribution:\n",
            "[('nlp', 3), ('and', 3), ('language', 2), ('is', 2), ('of', 2), ('natural', 1), ('processing', 1), ('a', 1), ('fascinating', 1), ('field', 1)]\n",
            "\n",
            "Full Frequency List:\n",
            "natural: 1\n",
            "language: 2\n",
            "processing: 1\n",
            "nlp: 3\n",
            "is: 2\n",
            "a: 1\n",
            "fascinating: 1\n",
            "field: 1\n",
            "that: 1\n",
            "combines: 1\n",
            "linguistics: 1\n",
            "computer: 1\n",
            "science: 1\n",
            "and: 3\n",
            "artificial: 1\n",
            "intelligence: 1\n",
            "it: 1\n",
            "enables: 1\n",
            "machines: 1\n",
            "to: 1\n",
            "understand: 1\n",
            "interpret: 1\n",
            "generate: 1\n",
            "human: 1\n",
            "applications: 1\n",
            "of: 2\n",
            "include: 1\n",
            "chatbots: 1\n",
            "sentiment: 1\n",
            "analysis: 1\n",
            "machine: 1\n",
            "translation: 1\n",
            "as: 1\n",
            "technology: 1\n",
            "advances: 1\n",
            "the: 1\n",
            "role: 1\n",
            "in: 1\n",
            "modern: 1\n",
            "solutions: 1\n",
            "becoming: 1\n",
            "increasingly: 1\n",
            "critical: 1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Create a custom annotator using spaCy or NLTK that identifies and labels proper nouns in a given text."
      ],
      "metadata": {
        "id": "3uSV9yKN-5uC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "# Load the model (if available)\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "def custom_proper_noun_annotator(text):\n",
        "    doc = nlp(text)\n",
        "    proper_nouns = []\n",
        "    for token in doc:\n",
        "        if token.pos_ == 'PROPN':\n",
        "            proper_nouns.append((token.text, 'Proper Noun'))\n",
        "    return proper_nouns\n",
        "\n",
        "# Example text (using the paragraph from Question 7 for consistency)\n",
        "text = \"Natural Language Processing (NLP) is a fascinating field that combines linguistics, computer science, and artificial intelligence. It enables machines to understand, interpret, and generate human language. Applications of NLP include chatbots, sentiment analysis, and machine translation. As technology advances, the role of NLP in modern solutions is becoming increasingly critical.\"\n",
        "\n",
        "proper_nouns = custom_proper_noun_annotator(text)\n",
        "print(\"Proper Nouns:\", proper_nouns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ_lC3cUFQDK",
        "outputId": "f9b9a75b-d73d-47ee-d903-24c7c59b3c23"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proper Nouns: [('Natural', 'Proper Noun'), ('Language', 'Proper Noun'), ('Processing', 'Proper Noun'), ('NLP', 'Proper Noun'), ('NLP', 'Proper Noun'), ('NLP', 'Proper Noun')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Using Gensim, demonstrate how to train a simple Word2Vec model on the following dataset consisting of example sentences:"
      ],
      "metadata": {
        "id": "NGEvJwLT-5rO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMgme8PNFrlb",
        "outputId": "6ff1d575-8a1e-47c8-ec89-f834b0fbf31e"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.2)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.4.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.0)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m71.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "dataset = [\n",
        "    \"Natural language processing enables computers to understand human language\",\n",
        "    \"Word embeddings are a type of word representation that allows words with similar meaning to have similar representation\",\n",
        "    \"Word2Vec is a popular word embedding technique used in many NLP applications\",\n",
        "    \"Text preprocessing is a critical step before training word embeddings\",\n",
        "    \"Tokenization and normalization help clean raw text for modeling\"\n",
        "]\n",
        "\n",
        "tokenized_dataset = [sentence.lower().split() for sentence in dataset]\n",
        "\n",
        "model = Word2Vec(sentences=tokenized_dataset, vector_size=50, window=5, min_count=1, workers=4, sg=1, epochs=20)\n",
        "\n",
        "print(\"Vocabulary:\", list(model.wv.key_to_index.keys()))\n",
        "\n",
        "print(\"\\nSimilar to 'word':\", model.wv.most_similar('word'))\n",
        "\n",
        "print(\"\\nSimilar to 'language':\", model.wv.most_similar('language'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gNWme3vlFg7a",
        "outputId": "6f8b5e6c-7c0c-4615-d216-a9f2a1e5e78e"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary: ['word', 'a', 'text', 'is', 'similar', 'representation', 'embeddings', 'to', 'language', 'modeling', 'for', 'raw', 'clean', 'help', 'normalization', 'and', 'tokenization', 'training', 'before', 'step', 'critical', 'preprocessing', 'applications', 'nlp', 'many', 'in', 'used', 'technique', 'embedding', 'popular', 'word2vec', 'have', 'meaning', 'with', 'words', 'allows', 'that', 'of', 'type', 'are', 'human', 'understand', 'computers', 'enables', 'processing', 'natural']\n",
            "\n",
            "Similar to 'word': [('before', 0.2734066843986511), ('enables', 0.26125431060791016), ('meaning', 0.24966613948345184), ('normalization', 0.22074124217033386), ('nlp', 0.2000207155942917), ('are', 0.1829235851764679), ('raw', 0.17133577167987823), ('applications', 0.16520081460475922), ('popular', 0.1566496044397354), ('help', 0.1488247513771057)]\n",
            "\n",
            "Similar to 'language': [('used', 0.3111428916454315), ('for', 0.23686298727989197), ('meaning', 0.2165430784225464), ('of', 0.2031378597021103), ('to', 0.18372809886932373), ('and', 0.17752960324287415), ('text', 0.1382451057434082), ('allows', 0.1381111741065979), ('are', 0.13162541389465332), ('preprocessing', 0.11069220304489136)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Imagine you are a data scientist at a fintech startup. You’ve been tasked with analyzing customer feedback. Outline the steps you would take to clean, process, and extract useful insights using NLP techniques from thousands of customer reviews."
      ],
      "metadata": {
        "id": "OwS9RwL_-5oj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a data scientist at a fintech startup, analyzing customer feedback from thousands of reviews involves systematic NLP techniques to uncover actionable insights like common pain points, sentiment trends, and feature requests. Below is an outline of the steps, followed by sample Python code demonstrating key parts (using available libraries like pandas for data handling and gensim for basic embedding; advanced NLP would typically use NLTK/spaCy, but simulated here due to environment constraints).\n",
        "\n",
        "Outlined Steps:\n",
        "\n",
        "1. Data Collection and Loading: Gather reviews from sources like app stores, surveys, or databases. Load into a structured format (e.g., pandas DataFrame) for easy manipulation.\n",
        "\n",
        "2. Data Cleaning: Remove irrelevant elements such as HTML tags, emojis, URLs, or duplicates. Handle missing values and filter out non-textual data.\n",
        "\n",
        "3. Text Normalization and Preprocessing: Convert to lowercase, remove punctuation/numbers, expand contractions, and apply stemming/lemmatization. Remove stop words to focus on meaningful terms.\n",
        "\n",
        "4. Tokenization and Feature Extraction: Break text into tokens (words/phrases). Use techniques like TF-IDF for weighting or word embeddings (e.g., Word2Vec) for semantic representation.\n",
        "\n",
        "5. Analysis and Insight Extraction: Perform sentiment analysis (e.g., polarity scoring), topic modeling (e.g., LDA), or clustering to identify themes. Calculate metrics like average sentiment per product feature.\n",
        "\n",
        "6. Visualization and Reporting: Use plots (e.g., word clouds, bar charts) to visualize frequencies, sentiments, or topics. Derive insights like \"80% negative feedback on transaction fees\" and recommend actions.\n",
        "\n",
        "7. Iteration and Model Improvement: Validate results with manual checks, fine-tune models, and deploy for ongoing monitoring.\n",
        "\n"
      ],
      "metadata": {
        "id": "EqoRJ8cU-5ly"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sample Python Code (Using pandas and gensim for a small example dataset; assumes basic preprocessing without NLTK):"
      ],
      "metadata": {
        "id": "RZR52jkv-5jG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "from gensim.models import Word2Vec\n",
        "from collections import Counter  # For frequency/sentiment simulation\n",
        "\n",
        "# Step 1: Sample data (thousands of reviews would be loaded from CSV)\n",
        "reviews = [\n",
        "    \"The app crashes often during transactions.\",\n",
        "    \"Great user interface but high fees.\",\n",
        "    \"Excellent customer support and fast transfers.\",\n",
        "    \"Security features are top-notch.\",\n",
        "    \"Too many bugs in the latest update.\"\n",
        "]\n",
        "df = pd.DataFrame({'review': reviews})\n",
        "\n",
        "# Step 2-3: Cleaning and Normalization\n",
        "def clean_text(text):\n",
        "    text = text.lower()  # Lowercase\n",
        "    text = re.sub(r'[^a-z\\s]', '', text)  # Remove punctuation/numbers\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
        "    return text\n",
        "\n",
        "df['cleaned'] = df['review'].apply(clean_text)\n",
        "\n",
        "# Step 4: Tokenization (simple split) and Word2Vec for embeddings\n",
        "tokenized = [text.split() for text in df['cleaned']]\n",
        "model = Word2Vec(sentences=tokenized, vector_size=10, window=3, min_count=1, workers=4, epochs=10)\n",
        "\n",
        "# Step 5: Extract Insights (e.g., word frequency for themes, simulated sentiment)\n",
        "word_freq = Counter(word for tokens in tokenized for word in tokens)\n",
        "print(\"Common Words/Themes:\", word_freq.most_common(5))\n",
        "\n",
        "# Simulated sentiment (positive/negative based on keywords; in reality, use VADER or classifier)\n",
        "positive_keywords = ['great', 'excellent', 'topnotch', 'fast']\n",
        "negative_keywords = ['crashes', 'high', 'bugs', 'often']\n",
        "df['sentiment'] = df['cleaned'].apply(lambda x: 'positive' if any(k in x for k in positive_keywords) else ('negative' if any(k in x for k in negative_keywords) else 'neutral'))\n",
        "print(\"\\nSentiment Distribution:\\n\", df['sentiment'].value_counts())\n",
        "\n",
        "# Example Insight: Similar words to 'fees' for related complaints\n",
        "try:\n",
        "    print(\"\\nSimilar to 'fees':\", model.wv.most_similar('fees'))\n",
        "except KeyError:\n",
        "    print(\"\\n'fees' not in vocabulary (small dataset).\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gUecKegNGnzs",
        "outputId": "6db8e848-5376-42ba-b21f-77a3495bfedf"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Common Words/Themes: [('the', 2), ('app', 1), ('crashes', 1), ('often', 1), ('during', 1)]\n",
            "\n",
            "Sentiment Distribution:\n",
            " sentiment\n",
            "positive    3\n",
            "negative    2\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Similar to 'fees': [('security', 0.27360907196998596), ('high', 0.18073837459087372), ('the', 0.17394468188285828), ('user', 0.13758254051208496), ('features', 0.10865871608257294), ('in', 0.09499213844537735), ('transactions', 0.04095911607146263), ('update', -0.03519481420516968), ('during', -0.050490282475948334), ('often', -0.06517450511455536)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "RAWYF95q-5Mf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hEYRn5v-2Zy"
      },
      "outputs": [],
      "source": []
    }
  ]
}