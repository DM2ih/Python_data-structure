{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# THEORITICAL QUESTIONS"
      ],
      "metadata": {
        "id": "mu3IKMc_qEJx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. What is TensorFlow 2.0, and how is it different from TensorFlow 1.x?"
      ],
      "metadata": {
        "id": "DIlQOHQFqC5M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorFlow 2.0 is an open-source deep learning framework developed by Google, released in September 2019. It focuses on ease of use, flexibility, and production-ready deployment for building and training machine learning models, particularly neural networks. It supports a wide range of tasks, including image classification, natural language processing, and reinforcement learning, with built-in support for GPUs and TPUs for accelerated computation.\n",
        "Key differences from TensorFlow 1.x include:\n",
        "\n",
        "* Eager Execution by Default: In TF 1.x, models were built using static computation graphs, requiring sessions to run operations. TF 2.0 uses eager execution, allowing immediate evaluation of operations, making debugging and development more intuitive, similar to Python scripting.\n",
        "\n",
        "* Simplified API: TF 2.0 integrates Keras as the high-level API, reducing boilerplate code. In 1.x, users often dealt with low-level APIs like tf.Session and tf.Placeholder.\n",
        "\n",
        "* tf.function for Graph Mode: TF 2.0 introduces tf.function to convert Python functions into optimized graphs for performance, bridging eager and graph modes.\n",
        "\n",
        "* Better Integration and Cleanup: Removed redundant APIs, improved error messages, and enhanced support for custom models and distributed training. Overall, TF 2.0 prioritizes developer productivity while maintaining performance."
      ],
      "metadata": {
        "id": "0MaWDRvRqCvu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "How do you install TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "qik4N6kbqClm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorFlow 2.0 can be installed via pip, Python's package manager. First, ensure you have Python 3.5–3.8 (as TF 2.0 has specific compatibility). Use a virtual environment for isolation:"
      ],
      "metadata": {
        "id": "KK44ULjkqCbt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "python -m venv tf_env\n",
        "source tf_env/bin/activate  # On Windows: tf_env\\Scripts\\activate"
      ],
      "metadata": {
        "id": "bIOouAdTrFyl"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Then install:"
      ],
      "metadata": {
        "id": "o6Iq2O62qCR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow==2.0.0"
      ],
      "metadata": {
        "id": "6HB0g7QYrZjo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For GPU support (requires CUDA and cuDNN):"
      ],
      "metadata": {
        "id": "pfaqrvV2qCIQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install tensorflow-gpu==2.0.0"
      ],
      "metadata": {
        "id": "DKwEKcJWriD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Note:** As of 2025, TF 2.0 is outdated; current versions are 2.17+, but for exact 2.0, specify the version. Verify with\n",
        "\n",
        "import tensorflow as tf;\n",
        "\n",
        "print(tf.__version__)\n",
        "\n",
        "in Python."
      ],
      "metadata": {
        "id": "xthpAg-UqB_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. What is the primary function of the tf.function in TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "kgs3esbaqB1O"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The primary function of tf.function is to convert a Python function into a TensorFlow graph, enabling graph execution mode for performance optimization. In TF 2.0's eager execution, operations run immediately, but for production or speed-critical scenarios, tf.function compiles the function into a callable graph that can be optimized, traced, and executed faster, especially with AutoGraph for control flow conversion (e.g., loops, conditionals). It improves efficiency by fusing operations, reducing Python overhead, and enabling portability (e.g., saving as SavedModel). Example:"
      ],
      "metadata": {
        "id": "STHU3TAuqBqV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "@tf.function\n",
        "def add(a, b):\n",
        "    return a + b\n",
        "result = add(tf.constant(1.0), tf.constant(2.0))  # Runs in graph mode\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "02jEiaIvsagk",
        "outputId": "29dfc5c6-d8da-4666-a34e-8f4b71e2820f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(3.0, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. What is the purpose of the Model class in TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "vzwza0pXqBhM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The tf.keras.Model class serves as a container for layers and defines the forward pass of a neural network. It allows building complex models by subclassing, enabling custom training loops, multiple inputs/outputs, and shared layers. It handles weight management, compilation (with optimizer, loss, metrics), training (fit()), evaluation, and prediction. It's part of the Keras API, making model definition modular and reusable. Example:"
      ],
      "metadata": {
        "id": "sN-Sv8bKqBXV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "class MyModel(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.dense = tf.keras.layers.Dense(10)\n",
        "    def call(self, inputs):\n",
        "        return self.dense(inputs)\n",
        "model = MyModel()"
      ],
      "metadata": {
        "id": "qF6KG2exswXy"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. How do you create a neural network using TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "XJBR3m5gqBOM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "To create a neural network, use the Keras Sequential API for simple stacks or subclass tf.keras.Model for complex ones. Steps:\n",
        "\n",
        "* Import: import tensorflow as tf\n",
        "\n",
        "* Define model:"
      ],
      "metadata": {
        "id": "oM0hB1fdqBFH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "d9lznq_DtQ7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Compile: model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "* Train: model.fit(x_train, y_train, epochs=10)\n",
        "\n",
        "* Evaluate/Predict: model.evaluate(x_test, y_test) or model.predict(new_data)\n",
        "This example is for a classification network (e.g., MNIST)."
      ],
      "metadata": {
        "id": "Z8lGo-fIqA7c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. What is the importance of Tensor Space in TensorFlow?"
      ],
      "metadata": {
        "id": "wqJdhTGpqAx0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorSpace (or TensorSpace.js) is a neural network 3D visualization framework that allows interactive exploration of deep learning models in the browser. Its importance in TensorFlow lies in enabling visual understanding of model architecture, intermediate layer activations, and data flow. It supports pre-trained TensorFlow models (via TF.js conversion), helping developers debug, educate, and interpret complex networks. For instance, users can rotate, zoom, and inspect tensors in 3D space, revealing insights into convolutions or attention mechanisms. It's integrative with TF for better model transparency."
      ],
      "metadata": {
        "id": "2gL1f0oYqAof"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How can TensorBoard be integrated with TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "_AZexagJqAfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorBoard is a visualization toolkit for monitoring training. Integrate by:\n",
        "\n",
        "* Import: from tensorflow.keras.callbacks import TensorBoard\n",
        "\n",
        "* Create callback: tensorboard_callback = TensorBoard(log_dir='logs/')\n",
        "\n",
        "* Pass to fit: model.fit(..., callbacks=[tensorboard_callback])\n",
        "\n",
        "* Launch: tensorboard --logdir logs/ in terminal, view at http://localhost:6006.\n",
        "It logs scalars (loss/accuracy), histograms, graphs, images, and embeddings for real-time insights."
      ],
      "metadata": {
        "id": "YDQfwM78qAWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. What is the purpose of TensorFlow Playground?"
      ],
      "metadata": {
        "id": "7XQMAWYSqAMn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "TensorFlow Playground is an interactive web-based tool (playground.tensorflow.org) for experimenting with neural networks without coding. Its purpose is educational: users adjust parameters like layers, neurons, learning rate, activation functions, and regularization to see how they affect training on simple datasets (e.g., classification problems). It visualizes decision boundaries, weights, and gradients in real-time, helping beginners understand concepts like overfitting, backpropagation, and optimization."
      ],
      "metadata": {
        "id": "eAG7ccyXqAC1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. What is Netron, and how is it useful for deep learning models?"
      ],
      "metadata": {
        "id": "zID0hVNap_46"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Netron is an open-source viewer for neural network models, supporting formats like TensorFlow SavedModel, Keras HDF5, PyTorch ONNX, and more. It's useful for inspecting model architecture, layers, weights, and operations visually without running the model. Developers use it to debug, document, and share models, revealing input/output shapes, parameters, and graph structures. It's cross-platform (desktop/web) and aids in model portability and understanding."
      ],
      "metadata": {
        "id": "CalyEoAsp_vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. What is the difference between TensorFlow and PyTorch?"
      ],
      "metadata": {
        "id": "unpl8-Xdp_lq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "* Design Philosophy: TensorFlow (Google) emphasizes production deployment with static graphs (via tf.function); PyTorch (Meta) focuses on research with dynamic graphs and eager execution for flexibility.\n",
        "\n",
        "* API: TF integrates Keras for high-level; PyTorch uses torch.nn for modular, Pythonic code.\n",
        "\n",
        "* Graph Mode: TF has both eager and graph; PyTorch is primarily dynamic but supports TorchScript for static.\n",
        "\n",
        "* Ecosystem: TF has better mobile/edge support (TF Lite); PyTorch excels in research with easier debugging.\n",
        "\n",
        "* Adoption: TF for industry scalability; PyTorch for academia. Both support distributed training, but PyTorch is often seen as more intuitive."
      ],
      "metadata": {
        "id": "LM6nASqVp_Z9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "11. How do you install PyTorch?"
      ],
      "metadata": {
        "id": "AxE2h7lvp_Pa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Install via pip or conda. Visit pytorch.org for tailored commands. Example for CPU:"
      ],
      "metadata": {
        "id": "D7yDYW-qp_FX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio"
      ],
      "metadata": {
        "id": "eW8x6qH0u2ty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For GPU (CUDA 12.4):"
      ],
      "metadata": {
        "id": "CsjUpGnWp-8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124"
      ],
      "metadata": {
        "id": "_TAX29Bnu9oB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Verify:**\n",
        "\n",
        "import torch;\n",
        "\n",
        "print(torch.__version__);\n",
        "\n",
        " print(torch.cuda.is_available()).\n",
        "\n",
        " As of 2025, current version is 2.4+."
      ],
      "metadata": {
        "id": "4RFXEM2sp-x7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "12. What is the basic structure of a PyTorch neural network?"
      ],
      "metadata": {
        "id": "2yLsfBtup-nW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "A PyTorch NN subclasses torch.nn.Module:\n",
        "\n",
        "\n",
        "* __ init __: Define layers (e.g., self.fc1 = nn.Linear(in_features, out_features)).\n",
        "\n",
        "* forward: Define computation (e.g., x = F.relu(self.fc1(x))).\n",
        "Example:"
      ],
      "metadata": {
        "id": "usHgqJYSp-cz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(784, 64)\n",
        "        self.fc2 = nn.Linear(64, 10)\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.fc1(x))\n",
        "        return self.fc2(x)\n",
        "model = Net()"
      ],
      "metadata": {
        "id": "rwKlEwE8vjqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "13. What is the significance of tensors in PyTorch?"
      ],
      "metadata": {
        "id": "PX8cGi10p-TH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Tensors are the core data structure in PyTorch, similar to NumPy arrays but with GPU acceleration and autograd support for automatic differentiation. They represent multi-dimensional data (scalars, vectors, matrices) and enable efficient computations. Significance: They track gradients for backpropagation, support broadcasting, and integrate with modules like torch.optim for training. Without tensors, building and optimizing NNs would be inefficient."
      ],
      "metadata": {
        "id": "61RPxt2Sp-Hf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "14. What is the difference between torch.Tensor and torch.cuda.Tensor in PyTorch?"
      ],
      "metadata": {
        "id": "6RR-biAKp99X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "* torch.Tensor: Default tensor on CPU, created with torch.tensor(data). Operations run on CPU.\n",
        "\n",
        "* torch.cuda.Tensor: Tensor on GPU for accelerated computation, created implicitly via .to('cuda') or torch.cuda.FloatTensor(). Requires CUDA-enabled GPU.\n",
        "Difference: Device placement—cuda.Tensor leverages GPU parallelism for faster training/inference; mixing them causes errors. Use tensor.to(device) for flexibility."
      ],
      "metadata": {
        "id": "kgoapPF-p9y3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "15. What is the purpose of the torch.optim module in PyTorch?"
      ],
      "metadata": {
        "id": "RAbZQFDdp9nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "torch.optim provides optimizers for updating model parameters during training. It implements algorithms like SGD, Adam, RMSprop to minimize loss via gradient descent. Purpose: Automates weight adjustments based on gradients from autograd. Example: optimizer = torch.optim.Adam(model.parameters(), lr=0.001); optimizer.step() after loss.backward()."
      ],
      "metadata": {
        "id": "aG3HICBqp9e5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "16. What are some common activation functions used in neural networks?"
      ],
      "metadata": {
        "id": "f19IADyKp9Uu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Activation functions introduce non-linearity. Common ones:\n",
        "\n",
        "\n",
        "* ReLU (Rectified Linear Unit): f(x) = max(0, x) – Fast, mitigates vanishing gradients.\n",
        "\n",
        "* Sigmoid: f(x) = 1 / (1 + e^{-x}) – For binary classification, but prone to vanishing gradients.\n",
        "\n",
        "* Tanh: f(x) = (e^x - e^{-x}) / (e^x + e^{-x}) – Zero-centered, better than sigmoid.\n",
        "\n",
        "* Softmax: For multi-class output probabilities.\n",
        "\n",
        "* Leaky ReLU: Variant of ReLU allowing small negative slope.\n",
        "\n",
        "* GELU/Swish: Used in transformers for better performance."
      ],
      "metadata": {
        "id": "-0kt_xtdp9Mm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "17. What is the difference between torch.nn.Module and torch.nn.Sequential in PyTorch?"
      ],
      "metadata": {
        "id": "FnMEWut4p9DR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "\n",
        "* torch.nn.Module: Base class for all NNs; requires subclassing with __init__ and forward. Flexible for custom logic, control flow, or non-sequential architectures.\n",
        "\n",
        "* torch.nn.Sequential: Container for stacking layers sequentially; no need for forward—it chains them automatically. Simpler for linear models but less flexible (no branching).\n",
        "Example: Sequential is like nn.Sequential(nn.Linear(10, 5), nn.ReLU()); Module allows conditional forwarding."
      ],
      "metadata": {
        "id": "UYK8zs2Cp85t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "18. How can you monitor training progress in TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "J-RrrfNep8vi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use callbacks like TensorBoard (see question 7), or print metrics during training. For custom:\n",
        "\n",
        "\n",
        "* In model.fit(..., verbose=1) for progress bars.\n",
        "\n",
        "* Custom loop: Use tf.keras.metrics and log with print or tf.summary.\n",
        "\n",
        "* TensorBoard for visuals, or integrate with tqdm for bars in loops."
      ],
      "metadata": {
        "id": "8MnViTFJp8mb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "19. How does the Keras API fit into TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "Gnr_cu0vwuwc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Keras is fully integrated as tf.keras in TF 2.0, serving as the official high-level API. It simplifies model building, compilation, and training while leveraging TF's low-level features. Users can mix Keras with core TF ops, and it supports eager execution. This makes TF more accessible, as prior to 2.0, Keras was separate."
      ],
      "metadata": {
        "id": "mfVWjwg6wumx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "20. What is an example of a deep learning project that can be implemented using TensorFlow 2.0?"
      ],
      "metadata": {
        "id": "clBQkr0BwuUW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Image classification on MNIST: Load data with tf.keras.datasets.mnist, build a CNN with Sequential (Conv2D, MaxPooling2D, Dense), compile with Adam and categorical_crossentropy, train with fit(), and evaluate. This demonstrates data loading, model architecture, training, and deployment basics. Other examples: GANs for image generation or RNNs for text prediction."
      ],
      "metadata": {
        "id": "kjULq8c7wuIo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "21. What is the main advantage of using pre-trained models in TensorFlow and PyTorch?"
      ],
      "metadata": {
        "id": "V0WGe6qQwt2J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "The main advantage is transfer learning: Pre-trained models (e.g., ResNet on ImageNet) provide learned features, reducing training time/data needs and improving performance on new tasks. Fine-tune by replacing final layers; saves compute resources and combats overfitting on small datasets."
      ],
      "metadata": {
        "id": "4_yl3-Z0wtLe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# PRACTICAL QUESTIONS"
      ],
      "metadata": {
        "id": "uANQaXEsp8bk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. How do you install and verify that TensorFlow 2.0 was installed successfully?"
      ],
      "metadata": {
        "id": "GoMYiq4EqSvx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "See first set question 2 for installation. Verify:"
      ],
      "metadata": {
        "id": "imO1wIoUx73q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "print(tf.__version__)\n",
        "print(tf.reduce_sum(tf.random.normal([1000, 1000])))  # Test operation"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WoJJ9ynmqSfY",
        "outputId": "53f11cec-a17a-4759-a6d7-4f9307f06a83"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.19.0\n",
            "tf.Tensor(668.4546, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "If GPU: print(tf.config.list_physical_devices('GPU'))."
      ],
      "metadata": {
        "id": "WJUXiXiiyAHC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. How can you define a simple function in TensorFlow 2.0 to perform addition?"
      ],
      "metadata": {
        "id": "kZMbr-ykyJ_P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Use eager mode or tf.function:"
      ],
      "metadata": {
        "id": "FM5peRGcqSO-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "def add(a, b):\n",
        "    return tf.add(a, b)\n",
        "# Or with tf.function\n",
        "@tf.function\n",
        "def add_graph(a, b):\n",
        "    return a + b\n",
        "result = add(tf.constant(3), tf.constant(4))  # Tensor(7)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MHBe7ZqaqR_3",
        "outputId": "4233ea41-5e60-44d4-d6f2-bfe0c7aefe49"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(7, shape=(), dtype=int32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. How can you create a simple neural network in TensorFlow 2.0 with one hidden layer?"
      ],
      "metadata": {
        "id": "47KFCj3LqRvr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu'),  # Hidden layer\n",
        "    tf.keras.layers.Dense(10, activation='softmax')  # Output layer, assuming 10 output classes\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy')"
      ],
      "metadata": {
        "id": "lbibr7bjqRdQ"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Train as in question 5."
      ],
      "metadata": {
        "id": "xig89exezLkx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. How can you visualize the training progress using TensorFlow and Matplotlib?"
      ],
      "metadata": {
        "id": "IiJRNMRMzLbB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Collect history from history = model.fit(...), then plot:"
      ],
      "metadata": {
        "id": "vxU6ZhnTqRK0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.plot(history.history['loss'], label='loss')\n",
        "plt.plot(history.history['val_loss'], label='val_loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "JW1PP5l_qQ3m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "For real-time, use callbacks or loop with plt updates."
      ],
      "metadata": {
        "id": "SHtjlXv7zka_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "5. How do you install PyTorch and verify the PyTorch installation?"
      ],
      "metadata": {
        "id": "qfBXNMNgzkPm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "See first set question 11 for installation. Verify:"
      ],
      "metadata": {
        "id": "--33RW_XqQh_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "print(torch.__version__)\n",
        "print(torch.tensor([1, 2, 3]).sum())  # tensor(6)\n",
        "print(torch.cuda.is_available())  # For GPU"
      ],
      "metadata": {
        "id": "X9_k_tFhqQSE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. How do you create a simple neural network in PyTorch?\n",
        "\n",
        "\n",
        "* See first set question 12 for structure and example."
      ],
      "metadata": {
        "id": "2u9vkDFLqQA1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. How do you define a loss function and optimizer in PyTorch?"
      ],
      "metadata": {
        "id": "NRx1k6wwqPgF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Loss: Use torch.nn modules, e.g., criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "* Optimizer: optimizer = torch.optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "* In training: loss = criterion(outputs, targets); loss.backward(); optimizer.step()"
      ],
      "metadata": {
        "id": "LA7d-oKD19nu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Assuming 'model' is already defined as a PyTorch model (e.g., an instance of torch.nn.Module or torch.nn.Sequential)\n",
        "# Example: from the answer to theoretical question 12:\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super().__init__()\n",
        "#         self.fc1 = nn.Linear(784, 64)\n",
        "#         self.fc2 = nn.Linear(64, 10)\n",
        "#     def forward(self, x):\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         return self.fc2(x)\n",
        "# model = Net()\n",
        "\n",
        "# Define a loss function\n",
        "# For classification tasks, CrossEntropyLoss is commonly used\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Define an optimizer\n",
        "# Adam and SGD are common optimizers\n",
        "# We pass the model's parameters to the optimizer\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Example of another optimizer: Stochastic Gradient Descent (SGD)\n",
        "# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
        "\n",
        "print(\"Loss function (criterion):\", criterion)\n",
        "print(\"Optimizer:\", optimizer)"
      ],
      "metadata": {
        "id": "CH8rTRKyqPP8"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. How do you implement a custom loss function in PyTorch?\n"
      ],
      "metadata": {
        "id": "46S_rdm8qO4B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Subclass nn.Module or define a function:"
      ],
      "metadata": {
        "id": "3NvufSm_0KkR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "class CustomLoss(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "    def forward(self, pred, target):\n",
        "        return (pred - target).pow(2).mean()  # MSE example\n",
        "criterion = CustomLoss()\n",
        "loss = criterion(outputs, targets)"
      ],
      "metadata": {
        "id": "f95ASQ8JqOnK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. How do you save and load a TensorFlow model?"
      ],
      "metadata": {
        "id": "vmYV45WBqN0b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* model.save('my_model.h5') (Keras format) or tf.saved_model.save(model, 'saved_model/')\n",
        "\n",
        "* Load: loaded_model = tf.keras.models.load_model('my_model.h5') or loaded = tf.saved_model.load('saved_model/')\n",
        "\n",
        "* For weights only: model.save_weights('weights.h5'); model.load_weights('weights.h5')"
      ],
      "metadata": {
        "id": "hqsEOr2m1p68"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4d04fe8"
      },
      "source": [
        "First, let's assume you have a trained TensorFlow Keras model named `model`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92ac68bd"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Assume 'model' is your trained TensorFlow Keras model\n",
        "# For demonstration, let's create a simple model\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Dense(64, activation='relu', input_shape=(784,)),\n",
        "    tf.keras.layers.Dense(10, activation='softmax')\n",
        "])\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Create a directory to save the model\n",
        "model_dir = 'my_tf_model'\n",
        "os.makedirs(model_dir, exist_ok=True)\n",
        "\n",
        "# Save the entire model in the TensorFlow SavedModel format\n",
        "model_path = os.path.join(model_dir, 'saved_model')\n",
        "model.save(model_path)\n",
        "\n",
        "print(f\"Model saved to: {model_path}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c27326e9"
      },
      "source": [
        "Now, let's load the saved model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dc5aac1"
      },
      "source": [
        "import tensorflow as tf\n",
        "import os\n",
        "\n",
        "# Define the path to the saved model\n",
        "model_dir = 'my_tf_model'\n",
        "model_path = os.path.join(model_dir, 'saved_model')\n",
        "\n",
        "# Load the model\n",
        "loaded_model = tf.keras.models.load_model(model_path)\n",
        "\n",
        "print(\"Model loaded successfully.\")\n",
        "\n",
        "# You can now use the loaded_model for prediction or further training\n",
        "# For example, let's print the model summary\n",
        "loaded_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}